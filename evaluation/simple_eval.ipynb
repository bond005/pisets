{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Callable, Literal\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import pipeline, Pipeline, WhisperProcessor\n",
    "\n",
    "from asr.asr import (\n",
    "    initialize_model_for_speech_segmentation,\n",
    "    initialize_model_for_speech_classification,\n",
    "    initialize_model_for_speech_recognition,\n",
    "    transcribe\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranscribeWhisperPipeline:\n",
    "    \"\"\"\n",
    "    A Whisper baseline to compare with `TranscribePisets`.\n",
    "    \"\"\"\n",
    "    def __init__(self, predictions_name: str):\n",
    "        self.predictions_name = predictions_name\n",
    "        self.whisper_pipeline = pipeline(\n",
    "            'automatic-speech-recognition',\n",
    "            model='openai/whisper-large-v3',\n",
    "            chunk_length_s=20,\n",
    "            stride_length_s=(4, 2),\n",
    "            device='cuda:0',\n",
    "            model_kwargs={'attn_implementation': 'sdpa'},\n",
    "            # torch_dtype=torch.float16,\n",
    "            generate_kwargs={\n",
    "                'language': '<|ru|>',\n",
    "                'task': 'transcribe',\n",
    "                'forced_decoder_ids': None\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def __call__(self, waveform: np.ndarray) -> dict[str, str]:\n",
    "        return self.whisper_pipeline(waveform)['text']\n",
    "\n",
    "\n",
    "class TranscribeWhisperLongform(TranscribeWhisperPipeline):\n",
    "    \"\"\"\n",
    "    A Whisper longform baseline to compare with `TranscribePisets`.\n",
    "    \"\"\"\n",
    "    def __init__(self, predictions_name: str, condition_on_prev_tokens: bool):\n",
    "        super().__init__(predictions_name)\n",
    "        self.whisper_processor = WhisperProcessor.from_pretrained(\n",
    "            'openai/whisper-large-v3',\n",
    "            language='Russian',\n",
    "            task='transcribe',\n",
    "        )\n",
    "        self.condition_on_prev_tokens = condition_on_prev_tokens\n",
    "    \n",
    "    def __call__(self, waveform: np.ndarray) -> dict[str, str]:\n",
    "        # https://github.com/huggingface/transformers/pull/27658\n",
    "        inputs = self.whisper_processor(\n",
    "            waveform,\n",
    "            return_tensors='pt',\n",
    "            truncation=False,\n",
    "            padding='longest',\n",
    "            return_attention_mask=True,  # probably we do not need this for Whisper\n",
    "            sampling_rate=16_000\n",
    "        )\n",
    "        result = self.whisper_pipeline.model.generate(\n",
    "            **inputs.to('cuda'),\n",
    "            condition_on_prev_tokens=self.condition_on_prev_tokens,\n",
    "            temperature=(0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n",
    "            logprob_threshold=-1.0,\n",
    "            compression_ratio_threshold=1.35,\n",
    "            return_timestamps=True,\n",
    "            language='<|ru|>',\n",
    "            task='transcribe',\n",
    "        )\n",
    "        return self.whisper_processor.batch_decode(result, skip_special_tokens=True)[0]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TranscribePisets:\n",
    "    \"\"\"\n",
    "    A Pisets wrapper for evaluation purposes.\n",
    "    \n",
    "    Transcribes waveform with Pisets and returns results for all stages.\n",
    "\n",
    "    In contrast to asr.asr.transcribe() this class:\n",
    "    - Concatenates transcriptions for all segments\n",
    "    - Does not return timestamps\n",
    "    - Allows to define custom names for all stages\n",
    "    \"\"\"\n",
    "    \n",
    "    segmenter: Pipeline | Callable\n",
    "    vad: Pipeline | Callable | Literal['skip']\n",
    "    asr: Pipeline | Callable | Literal['skip']\n",
    "\n",
    "    min_segment_size: int = 1\n",
    "    max_segment_size: int = 20\n",
    "    stretch: tuple[int, int] | None = None\n",
    "\n",
    "    segmenter_predictions_name: str | None = None\n",
    "    asr_predictions_name: str | None = None\n",
    "    asr_stretched_predictions_name: str | None = None\n",
    "    \n",
    "    def __call__(self, waveform: np.ndarray) -> dict[str, str]:\n",
    "        # transcribing\n",
    "        outputs = transcribe(\n",
    "            waveform,\n",
    "            segmenter=self.segmenter,\n",
    "            voice_activity_detector=(\n",
    "                self.vad\n",
    "                if self.vad != 'skip'\n",
    "                else (lambda audio: [{'score': 1, 'label': 'Speech'}])\n",
    "            ),\n",
    "            asr=(\n",
    "                self.asr\n",
    "                if self.asr != 'skip'\n",
    "                else (lambda audio: {'text': ''})\n",
    "            ),\n",
    "            min_segment_size=self.min_segment_size,\n",
    "            max_segment_size=self.max_segment_size,\n",
    "            stretch=self.stretch,\n",
    "        )\n",
    "        # concatenating segments\n",
    "        results = {}\n",
    "        if self.segmenter_predictions_name is not None:\n",
    "            results[self.segmenter_predictions_name] = [s.transcription_from_segmenter for s in outputs]\n",
    "        if self.asr_predictions_name is not None:\n",
    "            results[self.asr_predictions_name] = [s.transcription for s in outputs]\n",
    "        if self.asr_stretched_predictions_name is not None:\n",
    "            results[self.asr_stretched_predictions_name] = [s.transcription_stretched for s in outputs]\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining transcribers without instantiating them all at once to save GPU memory\n",
    "\n",
    "transcribers = {\n",
    "    'Whisper pipeline': lambda: TranscribeWhisperPipeline(\n",
    "        predictions_name='Baseline Whisper pipeline',\n",
    "    ),\n",
    "    'Whisper longform': lambda: TranscribeWhisperLongform(\n",
    "        predictions_name='Baseline Whisper longform',\n",
    "        condition_on_prev_tokens=False,\n",
    "    ),\n",
    "    'Whisper longform conditioned': lambda: TranscribeWhisperLongform(\n",
    "        predictions_name='Baseline Whisper longform conditioned',\n",
    "        condition_on_prev_tokens=True,\n",
    "    ),\n",
    "    'Pisets (segments 1s-20s)': lambda: TranscribePisets(\n",
    "        segmenter=initialize_model_for_speech_segmentation('ru', 'bond005/wav2vec2-large-ru-golos-with-lm'),\n",
    "        vad=initialize_model_for_speech_classification(),\n",
    "        asr=initialize_model_for_speech_recognition('ru', 'openai/whisper-large-v3'),\n",
    "        min_segment_size=1,\n",
    "        max_segment_size=20,\n",
    "        stretch=(3, 4),\n",
    "        segmenter_predictions_name='W2V2 Golos LM',\n",
    "        asr_predictions_name='Pisets WhisperV3 (segments 1s-20s)',\n",
    "        asr_stretched_predictions_name='Pisets WhisperV3 stretched (segments 1s-20s)',\n",
    "    ),\n",
    "    'Pisets (segments 10s-30s)': lambda: TranscribePisets(\n",
    "        segmenter=initialize_model_for_speech_segmentation('ru', 'bond005/wav2vec2-large-ru-golos-with-lm'),\n",
    "        vad=initialize_model_for_speech_classification(),\n",
    "        asr=initialize_model_for_speech_recognition('ru', 'openai/whisper-large-v3'),\n",
    "        min_segment_size=10,\n",
    "        max_segment_size=30,\n",
    "        asr_predictions_name='Pisets WhisperV3 (segments 10s-30s)',\n",
    "    ),\n",
    "    'W2V2 golos no LM': lambda: TranscribePisets(\n",
    "        segmenter=initialize_model_for_speech_segmentation('ru', 'bond005/wav2vec2-large-ru-golos'),\n",
    "        vad='skip',\n",
    "        asr='skip',\n",
    "        segmenter_predictions_name='W2V2 Golos no LM',\n",
    "    ),\n",
    "    'Pisets Podlodka': lambda: TranscribePisets(\n",
    "        segmenter=initialize_model_for_speech_segmentation('ru', 'bond005/wav2vec2-large-ru-golos-with-lm'),\n",
    "        vad=initialize_model_for_speech_classification(),\n",
    "        asr=initialize_model_for_speech_recognition('ru', 'bond005/whisper-large-v3-ru-podlodka'),\n",
    "        min_segment_size=1,\n",
    "        max_segment_size=20,\n",
    "        asr_predictions_name='Pisets WhisperV3 Podlodka (segments 1s-20s)',\n",
    "    ),\n",
    "    'Pisets no-VAD': lambda: TranscribePisets(\n",
    "        segmenter=initialize_model_for_speech_segmentation('ru', 'bond005/wav2vec2-large-ru-golos-with-lm'),\n",
    "        vad='skip',\n",
    "        asr=initialize_model_for_speech_recognition('ru', 'openai/whisper-large-v3'),\n",
    "        min_segment_size=1,\n",
    "        max_segment_size=20,\n",
    "        asr_predictions_name='Pisets WhisperV3 no-VAD (segments 1s-20s)',\n",
    "    ),\n",
    "    'Pisets no-VAD Podlodka': lambda: TranscribePisets(\n",
    "        segmenter=initialize_model_for_speech_segmentation('ru', 'bond005/wav2vec2-large-ru-golos-with-lm'),\n",
    "        vad='skip',\n",
    "        asr=initialize_model_for_speech_recognition('ru', 'bond005/whisper-large-v3-ru-podlodka'),\n",
    "        min_segment_size=1,\n",
    "        max_segment_size=20,\n",
    "        asr_predictions_name='Pisets WhisperV3 no-VAD Podlodka (segments 1s-20s)',\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    load_dataset('dangrebenkin/long_audio_youtube_lectures')\n",
    "    .cast_column('audio', Audio(sampling_rate=16_000))\n",
    "    ['train']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path('/home/oleg/pisets_test_results')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/oleg/pisets_test_results/zaliznyak Whisper pipeline.json\n",
      "GPU max allocated memory: 6.37 GB\n",
      "/home/oleg/pisets_test_results/harvard Whisper pipeline.json\n",
      "GPU max allocated memory: 6.37 GB\n",
      "/home/oleg/pisets_test_results/savvateev Whisper pipeline.json\n",
      "GPU max allocated memory: 6.36 GB\n",
      "/home/oleg/pisets_test_results/zhirinovsky Whisper pipeline.json\n",
      "GPU max allocated memory: 6.37 GB\n",
      "/home/oleg/pisets_test_results/lankov Whisper pipeline.json\n",
      "GPU max allocated memory: 6.37 GB\n",
      "/home/oleg/pisets_test_results/kolodezev Whisper pipeline.json\n",
      "GPU max allocated memory: 6.36 GB\n",
      "/home/oleg/pisets_test_results/tuberculosis Whisper pipeline.json\n",
      "GPU max allocated memory: 6.37 GB\n",
      "/home/oleg/pisets_test_results/zaliznyak Whisper longform.json\n",
      "GPU max allocated memory: 6.37 GB\n",
      "/home/oleg/pisets_test_results/harvard Whisper longform.json\n",
      "GPU max allocated memory: 6.39 GB\n",
      "/home/oleg/pisets_test_results/savvateev Whisper longform.json\n"
     ]
    }
   ],
   "source": [
    "for transcriber_name, transcriber_lambda in transcribers.items():\n",
    "\n",
    "    # instantiate transcriber on GPU\n",
    "    transcriber = transcriber_lambda()\n",
    "\n",
    "    for sample in dataset:\n",
    "        print(filepath := output_dir / f'{sample[\"name\"]} {transcriber_name}.json')\n",
    "\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        if filepath.is_file():\n",
    "            print(f'{str(filepath)} already exists')\n",
    "            continue\n",
    "\n",
    "        start_time = time.time()\n",
    "        transcriptions = transcriber(sample['audio']['array'][:160_000])\n",
    "                                     \n",
    "        results = {\n",
    "            'audio_name': sample['name'],\n",
    "            'transcriber_name': transcriber_name,\n",
    "            'elapsed_time': time.time() - start_time,\n",
    "            'transcriptions': transcriptions,\n",
    "        }\n",
    "\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f)\n",
    "\n",
    "        print(f'GPU max allocated memory: {torch.cuda.max_memory_allocated(0) / 2**30:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
